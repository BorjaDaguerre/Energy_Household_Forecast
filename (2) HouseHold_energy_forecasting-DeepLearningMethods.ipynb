{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02d2c08",
   "metadata": {},
   "source": [
    "### LSTM/GRU models \n",
    "\n",
    "Applying Deep Learning methods to predict one step-ahead daily energy household consumption. Data taken from [here](https://www.kaggle.com/datasets/jaganadhg/house-hold-energy-data/code). While notebook one implemented a [SARIMAX model](https://github.com/BorjaDaguerre/Energy_Household_Forecast/blob/main/(1)%20Household_energy_forecasting_SARIMAX.ipynb), here the goal is to leverage the non-linearily capacities of DL models to improve predictions. Model parameters have been searched for using validation data and different hyperparamters permutations, the lagged values considered for this methods have also been informed by the results of finding the 'p' and 'q' values in notebook 1 and the acf and pacf graphs present there. ALthough the dataset is maybe not long enough to demostrate benefits of using a DL model to capture compex time series features, the time series does not present very defined patterns (seasonality, trend) and could benefit from using the non-linear methods deployed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEAS:AUTOREGRESSIVE MODELS: comparing classification versus regression methods. predicting by day \n",
    "#(too short dataset ~720 datapoints)? 'reverse engineering' the cost of energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88720da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "import datetime as dt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Dropout \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142cce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, drop useless columns  \n",
    "\n",
    "df = pd.read_csv('D202.csv')\n",
    "df = df.drop(columns = ['NOTES','TYPE', 'UNITS'] )\n",
    "\n",
    "#number of years the time series covers\n",
    "len(np.unique(df['DATE']))/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data types of the dataframe\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with the data and the time, to create daily and hourly datasets \n",
    "\n",
    "df['FULL_DATE'] = ''\n",
    "\n",
    "for date in df['DATE'].items():\n",
    "    df['FULL_DATE'].loc[date[0]] = date [1] + ' ' + df['START TIME'].loc[date[0]] \n",
    "    \n",
    "df['FULL_DATE'] = pd.to_datetime(df['FULL_DATE'])\n",
    "df.index = df['FULL_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hourly and daily datasets for forecasting\n",
    "\n",
    "df['HOUR'] = [dt.replace(minute=0, second=0) for dt in df['FULL_DATE']]\n",
    "df['DATE'] = [date for date in df.index.date]\n",
    "\n",
    "daily_usage = df.groupby(['DATE']).agg({'USAGE':'sum'})\n",
    "hourly_usage = df.groupby(['HOUR']).agg({'USAGE':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate trainning, validation, and daily data test sets for hourly and daily data\n",
    "\n",
    "train_daily = daily_usage[:int(len(daily_usage) * 0.5)]\n",
    "train_hourly = hourly_usage[:int(len(hourly_usage) * 0.5)]\n",
    "validation_daily = daily_usage[int(len(daily_usage)* 0.5):-int(len(daily_usage) * 0.25)]\n",
    "validation_hourly = hourly_usage[int(len(hourly_usage)* 0.5):-int(len(hourly_usage) * 0.25)]\n",
    "test_daily = daily_usage[-int(len(daily_usage) * 0.25):]\n",
    "test_hourly = hourly_usage[-int(len(hourly_usage) * 0.25):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dinamically forecasting since ARIMA default model 'cheats'\n",
    "\n",
    "\n",
    "\n",
    "def one_step_ahead_DL(modelo, train_data, test_data, n_future, n_past, n_nodes_1, n_nodes_2, dropout, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Perform one-step-ahead dynamic forecasting using a deep learning model.\n",
    "\n",
    "    Parameters:\n",
    "        modelo: The type of model architecture to use (e.g., LSTM).\n",
    "        train_data (pd.DataFrame): Training data including both historical and validation data.\n",
    "        test_data (pd.DataFrame): Testing data.\n",
    "        n_future (int): Number of future steps to forecast.\n",
    "        n_past (int): Number of past steps to use for prediction.\n",
    "        n_nodes_1 (int): Number of nodes in the first LSTM layer.\n",
    "        n_nodes_2 (int): Number of nodes in the second LSTM layer.\n",
    "        dropout (float): Dropout rate.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        predictions (list): List of forecasted values.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    full_data = pd.concat([train_data, test_data], axis=0)\n",
    "    train_X, train_y, test_X = [], [], []\n",
    "\n",
    "    # Prepare training data\n",
    "    for i in range(n_past, len(full_data) - n_future + 1):\n",
    "        train_data_array = np.array(full_data)\n",
    "        train_X.append(train_data_array[i - n_past:i, 0:train_data_array.shape[1]])\n",
    "        train_y.append(train_data_array[i + n_future - 1:i + n_future, 0:train_data.shape[1]])\n",
    "\n",
    "    # Prepare testing data\n",
    "    for i in range(len(train_data), len(train_data) + len(test_data)):\n",
    "        full_data_array = np.array(full_data)\n",
    "        test_X.append(full_data_array[i - n_past:i, 0:test_data.shape[1]])\n",
    "\n",
    "    train_X, train_y, test_X = np.array(train_X), np.array(train_y), np.array(test_X)\n",
    "\n",
    "    # Initialize and compile the model\n",
    "    model = Sequential()\n",
    "    model.add(modelo(n_nodes_1, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(modelo(n_nodes_2, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train and forecast iteratively\n",
    "    for i in range(len(train_data) - n_past, len(full_data) - n_past):\n",
    "        model.fit(train_X[:i], train_y[:i], epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=False)\n",
    "        prediction = model.predict(test_X[i - train_X.shape[0]].reshape(1, train_X.shape[1], train_X.shape[2]))\n",
    "        predictions.append(prediction[0][0])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array creation for hyperarameter tunning and lags feed to the model\n",
    "\n",
    "full_data = pd.concat([train_daily,validation_daily],axis = 0)\n",
    "train_X, train_y, val_X, val_y = [], [], [], []\n",
    "\n",
    "n_past, n_future = 7, 1\n",
    "    \n",
    "for i in range(n_past, len(train_daily) - n_future + 1):\n",
    "    train_data = np.array(train_daily)\n",
    "    train_X.append(train_data[i - n_past:i, 0:train_data.shape[1]])\n",
    "    train_y.append(train_data[i + n_future-1:i+n_future,0:train_data.shape[1]])\n",
    "    \n",
    "for i in range(n_past, len(validation_daily) + - n_future + 1):\n",
    "    val_data = np.array(validation_daily)\n",
    "    val_X.append(val_data[i - n_past:i])\n",
    "    val_y.append(val_data[i + n_future-1:i+n_future])\n",
    "\n",
    "\n",
    "train_X, train_y, val_X, val_y = np.array(train_X), np.array(train_y), np.array(val_X), np.array(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70420c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function and nested loops for hyperparameter tunning forecasting the whole validation dataset. Hyperparameters consider\n",
    "# where informed two factors: the short lenght of the train set (366 observations) and its univariety while also having in mind\n",
    "# the irregular patterns present in them\n",
    "\n",
    "def create_model(modelo,n_nodes_1, n_nodes_2, dropout):\n",
    "    \"\"\"\n",
    "    Create a LSTM model with specified architecture.\n",
    "\n",
    "    Parameters:\n",
    "        n_nodes_1 (int): Number of nodes in the first GRU layer.\n",
    "        n_nodes_2 (int): Number of nodes in the second GRU layer.\n",
    "        dropout (float): Dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        model (Sequential): Compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(modelo(n_nodes_1, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(modelo(n_nodes_2, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparameters to tune\n",
    "n_nodes_1_values = [8, 16, 32]\n",
    "n_nodes_2_values = [8, 16, 32]\n",
    "dropout_values = [0.2, 0.5, 0.7]\n",
    "batch_size = [8, 16, 32]\n",
    "best_model = None\n",
    "best_nodes_1 = None\n",
    "best_nodes_2 = None\n",
    "best_dropout = None\n",
    "best_batch_size = None\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for n_nodes_1 in n_nodes_1_values:\n",
    "    for n_nodes_2 in n_nodes_2_values:\n",
    "        for dropout in dropout_values:\n",
    "            for batch in batch_size:\n",
    "                model = create_model(GRU, n_nodes_1, n_nodes_2, dropout)\n",
    "                history = model.fit(train_X, train_y, epochs=30, batch_size=batch, validation_data=(val_X, val_y), verbose=0)\n",
    "                val_loss = history.history['val_loss'][-1]\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_nodes_1 = n_nodes_1\n",
    "                    best_nodes_2 = n_nodes_2\n",
    "                    best_dropout = dropout\n",
    "                    best_batch_size = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042aaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the validation set to the train set for the final test\n",
    "\n",
    "train_full = pd.concat([train_daily, validation_daily], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over 10 iterations of the best validation LSTM results: #16 16 0.7 8 30 for 7 lagged values \n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    results_model = one_step_ahead_DL(LSTM, train_full, test_daily, 1, 7, 16 ,16, 0.7, 8, 30)\n",
    "    results.append(results_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3464206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average over the 10 predictions\n",
    "\n",
    "average_predictions_LSTM = []\n",
    "\n",
    "for idx in range(0, len(results[0])):\n",
    "    average = []\n",
    "    for lst in results:\n",
    "        average.append(lst[idx])\n",
    "    average_predictions_LSTM.append(np.mean(average))\n",
    "\n",
    "average_predictions_LSTM = pd.DataFrame(average_predictions_LSTM, index = test_daily.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE and predictions/real values plot for the LSTM model\n",
    "\n",
    "round(mean_absolute_error(average_predictions_LSTM, test_daily),3)\n",
    "\n",
    "plt.figure(figsize = (12,6), dpi = 1000)\n",
    "plt.plot(average_predictions_LSTM, label = 'predicted')\n",
    "plt.plot(test_daily, label = 'actual')\n",
    "plt.title('One-step ahead LSTM forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over 10 iterations of the best validation GRU results: #16 16 0.7 8 30\n",
    "\n",
    "results_GRU = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    m_results_GRU = one_step_ahead_DL(GRU, train_full, test_daily, 1, 7, 32 ,8, 0.2, 32, 30)\n",
    "    results_GRU.append(m_results_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average over the 10 predictions\n",
    "\n",
    "average_predictions_GRU = []\n",
    "\n",
    "for idx in range(0, len(results[0])):\n",
    "    average = []\n",
    "    for lst in results_GRU:\n",
    "        average.append(lst[idx])\n",
    "    average_predictions_GRU.append(np.mean(average))\n",
    "\n",
    "average_predictions_GRU = pd.DataFrame(average_predictions_GRU, index = test_daily.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9943e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "\n",
    "round(mean_absolute_error(average_predictions_GRU, test_daily),3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions/real values plot\n",
    "\n",
    "plt.figure(figsize = (12,6), dpi = 1000)\n",
    "plt.plot(average_predictions_GRU, label = 'predicted')\n",
    "plt.plot(test_daily, label = 'actual')\n",
    "plt.title('One-step ahead GRU forecast')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
